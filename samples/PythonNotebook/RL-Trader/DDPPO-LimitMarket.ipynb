{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from Environments.SineEnv import SineMarketEnv\n",
    "from Environments.LimitOrderMarket import LimitMarket\n",
    "from Environments.wrappers.reward_wrapper import CuriosityWrapper\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "import gym\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.agents.ppo.ddppo import DEFAULT_CONFIG\n",
    "from ray.rllib.agents.ppo.ddppo import DDPPOTrainer\n",
    "import os\n",
    "import gc\n",
    "import torch \n",
    "import pandas as pd\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ray.init(num_gpus = 1, num_cpus = 4)\n",
    "except:\n",
    "    ray.shutdown()\n",
    "    ray.init(num_gpus = 1, num_cpus = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_config = DEFAULT_CONFIG.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_config['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer_config['model']['use_lstm'] = False\n",
    "trainer_config['model']['lstm_cell_size'] = 512\n",
    "trainer_config['num_gpus'] = 0\n",
    "trainer_config['num_gpus_per_worker'] = 1\n",
    "trainer_config['num_envs_per_worker'] = 1\n",
    "trainer_config['gamma'] = 0\n",
    "trainer_config['entropy_coeff'] = 0\n",
    "trainer_config['framework'] = 'torch'\n",
    "trainer_config['num_workers'] = 1\n",
    "trainer_config['horizon'] = 1000\n",
    "trainer_config['rollout_fragment_length'] = 1000\n",
    "trainer_config['model']['framestack'] = False\n",
    "trainer_config['model']['fcnet_hiddens'] = [1024, 1024]\n",
    "conf = {'data': 'Data/ground_truth/',\n",
    "        'starting_money': 1000,\n",
    "        'starting_stocks': 0,\n",
    "        'episode_length': 1000,\n",
    "        'commission': 0.0025,\n",
    "        'state_orders_num': 10,\n",
    "        'max_horizon' : 100,\n",
    "        'curiosity_reward' : 0\n",
    "        }\n",
    "trainer_config['env_config'] = conf\n",
    "trainer_config['entropy_coeff_schedule'] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curiosity_env_create(env_config):\n",
    "    return CuriosityWrapper(LimitMarket(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('CuriosityLimitMarket', curiosity_env_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = DDPPOTrainer(trainer_config, env = 'CuriosityLimitMarket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_reward = -np.inf\n",
    "trainer.save()\n",
    "hall_of_fame = [0]\n",
    "last_checkpoint = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(100):\n",
    "    print(\"Training iteration {}...\".format(i))\n",
    "    results = trainer.train()\n",
    "    this_reward = results['episode_reward_max']\n",
    "    if this_reward > best_reward:\n",
    "        best_reward = this_reward\n",
    "        trainer.save()\n",
    "        path = trainer.logdir + 'checkpoint_{0}/checkpoint-{0}'.format(last_checkpoint)\n",
    "        os.remove(path)\n",
    "        last_checkpoint = i + 1\n",
    "        hall_of_fame.append(i+1)\n",
    "        print('New best reward')\n",
    "        print(best_reward)\n",
    "    if i % 10 == 0:\n",
    "        print('Best Reward So Far')\n",
    "        print(best_reward)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = trainer.logdir + 'checkpoint_{0}/checkpoint-{0}'.format(hall_of_fame[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training = pd.read_csv(trainer.logdir + 'progress.csv')\n",
    "plt.plot(training['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = LimitMarket(conf)\n",
    "obs = env.reset()\n",
    "\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "prices = []\n",
    "assets = []\n",
    "actions = []\n",
    "states = [obs]\n",
    "rewards = []\n",
    "hidden = [torch.zeros(512),torch.zeros(512)]\n",
    "infos = []\n",
    "while not done:\n",
    "    action, hidden, info = trainer.compute_action(obs, hidden)\n",
    "    obs, reward, done, results = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    assets.append(results['assets'])\n",
    "    prices.append(results['current_price'])\n",
    "    states.append(obs)\n",
    "    infos.append(info)\n",
    "    if i % 100 == 0:\n",
    "        print('Step: {}/{}'.format(i, 200))\n",
    "print(\"Cumulative reward you've received is: {}. Congratulations!\".format(cumulative_reward))\n",
    "print(\"Asset_Gain {}\".format(assets[-1] -assets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_actions = []\n",
    "for action in actions:\n",
    "    pure_actions.append(action[0])\n",
    "    \n",
    "actions = pure_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy10 = np.ma.masked_where(np.array(actions) != 0, prices)\n",
    "buy20 = np.ma.masked_where(np.array(actions) != 1, prices)\n",
    "buy50 = np.ma.masked_where(np.array(actions) != 2, prices)\n",
    "sell10 = np.ma.masked_where(np.array(actions) != 3, prices)\n",
    "sell20 = np.ma.masked_where(np.array(actions) != 4, prices)\n",
    "sell50 = np.ma.masked_where(np.array(actions) != 5, prices)\n",
    "hold = np.ma.masked_where(np.array(actions) != 6, prices)\n",
    "\n",
    "# plt.plot(prices, marker = '', markersize = 0.5, markevery = np.where(np.array(actions) == 6, True, False))\n",
    "# plt.figure(figsize = (20, 15))\n",
    "# plt.plot(buy10, c = 'turquoise', linewidth = 0.6)\n",
    "# plt.plot(buy20, c = 'lime', linewidth = 0.6)\n",
    "# plt.plot(buy50, c = 'green', linewidth = 0.6)\n",
    "# plt.plot(hold, c = 'blue', linewidth = 0.6)\n",
    "graph_prices = prices[::10][:500]\n",
    "graph_actions = actions[::10][:500]\n",
    "colors = ['r', 'r', 'r', 'g', 'g', 'g', 'b']\n",
    "fig = plt.figure(figsize = (10, 6))\n",
    "plt.scatter(range(len(graph_prices)), graph_prices, s=1, color = np.array(colors)[graph_actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "obs = env.test()\n",
    "\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "prices = []\n",
    "assets = []\n",
    "actions = []\n",
    "states = [obs]\n",
    "rewards = []\n",
    "hidden = [torch.zeros(512),torch.zeros(512)]\n",
    "infos = []\n",
    "market_beaters = []\n",
    "while not done:\n",
    "    action, hidden, info = trainer.compute_action(obs, hidden)\n",
    "    obs, reward, done, results = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    assets.append(results['assets'])\n",
    "    prices.append(results['current_price'])\n",
    "    states.append(obs)\n",
    "    infos.append(info)\n",
    "    market_beaters.append(results['market_beater'])\n",
    "print(\"Cumulative reward you've received is: {}. Congratulations!\".format(cumulative_reward))\n",
    "print(\"Asset_Gain {}\".format(assets[-1] -assets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(market_beaters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_actions = []\n",
    "for action in actions:\n",
    "    print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "buy10 = np.ma.masked_where(np.array(actions) == 0, prices)\n",
    "buy20 = np.ma.masked_where(np.array(actions) == 1, prices)\n",
    "buy50 = np.ma.masked_where(np.array(actions) == 2, prices)\n",
    "hold = np.ma.masked_where(np.array(actions) == 3, prices)\n",
    "\n",
    "plt.plot(buy10, c = 'turquoise')\n",
    "plt.plot(buy20, c = 'lime')\n",
    "plt.plot(buy50, c = 'green')\n",
    "plt.plot(hold, c = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_beaters[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_norm = preprocessing.normalize(np.array(prices).reshape(-1,1))\n",
    "assets_norm = preprocessing.normalize(np.array(assets).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_norm-prices_norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
