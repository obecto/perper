{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XppVMIB4-66O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "krGSuKzpWAiu"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger('rnn')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler('ss.log')\n",
    "fh.setLevel(logging.INFO)\n",
    "# create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "fh.setFormatter(formatter)\n",
    "# add the handlers to logger\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5Vl_ING3emGk"
   },
   "outputs": [],
   "source": [
    "data_whole = pd.read_csv('../Data/ground_truth/ground_truth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLJQTRl9IgZ_"
   },
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_h_up) - 100\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        l_h_up = self.labels[index + 100]\n",
    "\n",
    "        return item, l_h_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DQ7RdLGVYdN"
   },
   "outputs": [],
   "source": [
    "if case == 1: \n",
    "    sc = MinMaxScaler(feature_range = (-1,1))\n",
    "    train_data = sc.fit_transform(train_data)\n",
    "    val_data = sc.fit_transform(val_data)\n",
    "    test_data = sc.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwbvMZ_SVTPc"
   },
   "outputs": [],
   "source": [
    "if case == 1:\n",
    "    train_data = torch.tensor(train_data)\n",
    "    val_data = torch.tensor(val_data)\n",
    "    test_data = torch.tensor(test_data)\n",
    "    print(\"scaled data\")\n",
    "if case == 0:\n",
    "    train_data = torch.tensor(train_data.values)\n",
    "    val_data = torch.tensor(val_data.values)\n",
    "    test_data = torch.tensor(test_data.values)\n",
    "    print(\"not scaled data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9P-5_1fKJ3FV"
   },
   "outputs": [],
   "source": [
    "train_data_h = Data(train_data, train_l_h_merged)\n",
    "test_loader_d = DataLoader(test_data_d, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_d29-BX_bfL"
   },
   "outputs": [],
   "source": [
    "''' New activation function '''\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * (torch.tanh(F.softplus(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPbFzwZvdVfE"
   },
   "outputs": [],
   "source": [
    "''' Defining neural net '''\n",
    "\n",
    "class Linear_out(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense_1 = nn.Sequential(\n",
    "            nn.Linear(in_size, 64),\n",
    "            Mish(),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "        self.dense_2 = nn.Sequential(\n",
    "            nn.Linear(64, 10), \n",
    "            nn.Dropout(p=0)\n",
    "        )\n",
    "        self.dense_3 = nn.Linear(10, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        print(\"after dense 1: \", x[0])\n",
    "        x = self.dense_2(x)\n",
    "        print(\"after dense 2: \", x[0])\n",
    "        print(\"sanity check: \", x[0]-x[1]+x[2]-x[3])\n",
    "        x = self.dense_3(x)\n",
    "        print(\"after dense 3: \", x[:7])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWkjgdR1GVLN"
   },
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijah5B_qK9My"
   },
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation, padding_mode='zeros')\n",
    "        self.group_norm1 = nn.GroupNorm(out_channels, out_channels)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.mish1 = Mish()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation, padding_mode='zeros')\n",
    "        self.group_norm2 = nn.GroupNorm(out_channels, out_channels)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.mish2 = Mish()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net_1 = nn.Sequential(self.conv1, self.chomp1, self.group_norm1, self.mish1, self.dropout1)\n",
    "        self.net_2 = nn.Sequential(self.conv2, self.chomp2, self.group_norm2, self.mish2, self.dropout2)\n",
    "        self.mish = Mish()\n",
    "\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net_1(x)\n",
    "        # print(\"conv_output_1: \", out.size()) \n",
    "        out = self.net_2(out)\n",
    "        # print(\"conv output_2: \", out.size()) \n",
    "        res = self.downsample(x)\n",
    "        # print(\"conv downsample: \", res.size()) \n",
    "        return self.mish(res + out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfBa_1KZny9h"
   },
   "outputs": [],
   "source": [
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=4, dilation_size=1):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "\n",
    "        self.group_norm1 = nn.GroupNorm(in_channels, in_channels)\n",
    "        self.group_norm2 = nn.GroupNorm(out_channels, out_channels)\n",
    "        self.mish1 = Mish()\n",
    "        self.conv_part = nn.Sequential(TemporalBlock(in_channels, 32, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                    padding=kernel_size-1),\n",
    "                                    TemporalBlock(32, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                    padding=kernel_size-1))\n",
    "        self.linear_part = Linear_out(out_channels*100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.group_norm1(x)\n",
    "        x = self.conv_part(x)\n",
    "        print(\"After conv part:\", x[0])\n",
    "        test = self.mish1(x)\n",
    "        print(\"After mish without group norm: \", test[0])\n",
    "        x = self.group_norm2(x)\n",
    "        x = self.mish1(x)\n",
    "        print(\"After mish with group norm: \", x[0])\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.linear_part(x)\n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qWIW3k2qGpO"
   },
   "outputs": [],
   "source": [
    "''' Eval function '''\n",
    "\n",
    "def Eval_epoch(model, dataloader, criterion, epoch):\n",
    "    losses = []\n",
    "\n",
    "    run_mean = []\n",
    "    \n",
    "    preds = {\n",
    "        \"h_up\":[]\n",
    "    }\n",
    "    labels = {\n",
    "        \"h_up\":[]\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            seqs, label_h_up = batch\n",
    "            h_up = model(seqs)\n",
    "            preds[\"h_up\"].append(h_up.cpu().numpy())\n",
    "            labels[\"h_up\"].append(label_h_up.cpu().numpy())\n",
    "\n",
    "            loss = criterion(h_up.float(), label_h_up.float())\n",
    "\n",
    "            losses.append(loss)\n",
    "\n",
    "            run_mean.append(sum(losses)/len(losses))\n",
    "\n",
    "            if step % 20 ==0:\n",
    "                logger.info(\"Eval epoch:{}    Step:{}/{}    loss:{:.4f}({:.4f})\".format(epoch, step, len(dataloader), loss, sum(losses)/len(losses)))\n",
    "                logger.debug(\"Output : {}    Label : {}\".format(h_up, label_h_up))\n",
    "\n",
    "    final_loss = sum(losses)/len(losses)\n",
    "    logger.info(\"Final loss: {:.4f}\".format(final_loss))\n",
    "\n",
    "    preds[\"h_up\"] = np.concatenate(preds[\"h_up\"])\n",
    "\n",
    "    labels[\"h_up\"] = np.concatenate(labels[\"h_up\"])\n",
    "\n",
    "    logger.info(\"r2 score: {:.5f}\".format(r2_score(labels['h_up'], preds['h_up'])))\n",
    "\n",
    "    return preds, labels, run_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5hady37rCTn"
   },
   "outputs": [],
   "source": [
    "''' Initiation of TCN '''\n",
    "\n",
    "net = TemporalConvNet(259, 64)\n",
    "net.double()\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.01)\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyVHwjXdxbDH"
   },
   "outputs": [],
   "source": [
    "''' Training cell for TCN '''\n",
    "\n",
    "loss_history = [None]*10\n",
    "for epoch in range(10):\n",
    "    loss_history[epoch] = []\n",
    "    net.train()\n",
    "    for step, batch in enumerate(train_loader_d):\n",
    "        input_data, labels = batch\n",
    "        out = net(input_data)\n",
    "        loss = criterion(out, labels)\n",
    "        loss_history[epoch].append(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if step % 40 == 0:\n",
    "            logger.info(\"Train epoch: {}    Step: {}/{}     mse: {}({})     prediction range: {}\".format(epoch, step, len(train_loader_d), loss, sum(loss_history[epoch])/len(loss_history[epoch]), max(out)-min(out)))\n",
    "            print(\"Output: {}\".format(out))\n",
    "            print(\"Labels: {}\".format(labels))\n",
    "        optimizer.step()\n",
    "    # net.eval() \n",
    "    predictions, labels, run_mean = Eval_epoch(net, val_loader_d, criterion, epoch)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, 'drive/My Drive/Auto ML/TCN_d_full_{}.pt'.format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDDpsDlEzEAt"
   },
   "outputs": [],
   "source": [
    "''' Load and test TCN '''\n",
    "net = TemporalConvNet(259, 64).double()\n",
    "if torch.cuda.is_available:\n",
    "    net.cuda()\n",
    "    map_location = None\n",
    "else:\n",
    "    map_location = 'cpu'\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "checkpoint = torch.load('drive/My Drive/Auto ML/TCN_d_full_4.pt', map_location=map_location)\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54DqsUzmz21E"
   },
   "outputs": [],
   "source": [
    "preds, labels, run_means = Eval_epoch(net, test_loader_d, nn.MSELoss(), epoch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9K3clGkGM5_"
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(labels[\"h_up\"])), labels[\"h_up\"]-preds[\"h_up\"], s=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pytorch TCN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
