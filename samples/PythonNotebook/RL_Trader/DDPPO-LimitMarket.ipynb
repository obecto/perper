{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Environments.BaseMarket import TestEnv\n",
    "from Environments.LimitOrderMarket import LimitMarket\n",
    "from Environments.wrappers.reward_wrapper import CuriosityWrapper\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "import gym\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.agents.ppo.ddppo import DEFAULT_CONFIG\n",
    "from ray.rllib.agents.ppo.ddppo import DDPPOTrainer\n",
    "import os\n",
    "import gc\n",
    "import torch \n",
    "import pandas as pd\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-09 13:44:10,580\tINFO services.py:1272 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ray.init(num_gpus = 1, num_cpus = 4)\n",
    "except:\n",
    "    ray.shutdown()\n",
    "    ray.init(num_gpus = 1, num_cpus = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_config = DEFAULT_CONFIG.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_use_default_native_models': False,\n",
       " 'fcnet_hiddens': [1024, 1024],\n",
       " 'fcnet_activation': 'tanh',\n",
       " 'conv_filters': None,\n",
       " 'conv_activation': 'relu',\n",
       " 'post_fcnet_hiddens': [],\n",
       " 'post_fcnet_activation': 'relu',\n",
       " 'free_log_std': False,\n",
       " 'no_final_linear': False,\n",
       " 'vf_share_layers': True,\n",
       " 'use_lstm': False,\n",
       " 'max_seq_len': 20,\n",
       " 'lstm_cell_size': 512,\n",
       " 'lstm_use_prev_action': False,\n",
       " 'lstm_use_prev_reward': False,\n",
       " '_time_major': False,\n",
       " 'use_attention': False,\n",
       " 'attention_num_transformer_units': 1,\n",
       " 'attention_dim': 64,\n",
       " 'attention_num_heads': 1,\n",
       " 'attention_head_dim': 32,\n",
       " 'attention_memory_inference': 50,\n",
       " 'attention_memory_training': 50,\n",
       " 'attention_position_wise_mlp_dim': 32,\n",
       " 'attention_init_gru_gate_bias': 2.0,\n",
       " 'attention_use_n_prev_actions': 0,\n",
       " 'attention_use_n_prev_rewards': 0,\n",
       " 'num_framestacks': 'auto',\n",
       " 'dim': 84,\n",
       " 'grayscale': False,\n",
       " 'zero_mean': True,\n",
       " 'custom_model': None,\n",
       " 'custom_model_config': {},\n",
       " 'custom_action_dist': None,\n",
       " 'custom_preprocessor': None,\n",
       " 'lstm_use_prev_action_reward': -1,\n",
       " 'framestack': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_config['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config['model']['use_lstm'] = False\n",
    "trainer_config['model']['lstm_cell_size'] = 512\n",
    "trainer_config['num_gpus'] = 0\n",
    "trainer_config['num_gpus_per_worker'] = 1\n",
    "trainer_config['num_envs_per_worker'] = 1\n",
    "trainer_config['gamma'] = 0\n",
    "# trainer_config['entropy_coeff'] = 0\n",
    "trainer_config['framework'] = 'torch'\n",
    "trainer_config['num_workers'] = 1\n",
    "trainer_config['horizon'] = 1000\n",
    "trainer_config['rollout_fragment_length'] = 1000\n",
    "trainer_config['model']['framestack'] = False\n",
    "trainer_config['model']['fcnet_hiddens'] = [1024, 1024]\n",
    "conf = {'data': 'Data/indicator_dataset/',\n",
    "        'starting_money': 1000,\n",
    "        'starting_stocks': 0,\n",
    "        'episode_length': 1000,\n",
    "        'commission': 0.0025,\n",
    "        'state_orders_num': 10,\n",
    "        'max_horizon' : 100,\n",
    "        'curiosity_reward' : 0\n",
    "        }\n",
    "trainer_config['env_config'] = conf\n",
    "# trainer_config['entropy_coeff_schedule'] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curiosity_env_create(env_config):\n",
    "    return TestEnv(LimitMarket(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('CuriosityLimitMarket', curiosity_env_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=10740)\u001b[0m 2021-06-09 13:44:33,479\tERROR worker.py:409 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=10740, ip=192.168.1.100)\n",
      "\u001b[2m\u001b[36m(pid=10740)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 501, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=10740)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=10740)\u001b[0m   File \"D:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=10740)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=10740)\u001b[0m   File \"D:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 536, in __init__\n",
      "\u001b[2m\u001b[36m(pid=10740)\u001b[0m     self.policy_map, self.preprocessors = self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(pid=10740)\u001b[0m   File \"D:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1196, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(pid=10740)\u001b[0m     policy_map[name] = cls(obs_space, act_space, merged_conf)\n",
      "\u001b[2m\u001b[36m(pid=10740)\u001b[0m   File \"D:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\policy\\policy_template.py\", line 236, in __init__\n",
      "\u001b[2m\u001b[36m(pid=10740)\u001b[0m     self.model, dist_class = make_model_and_action_dist(\n",
      "\u001b[2m\u001b[36m(pid=10740)\u001b[0m   File \"D:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\agents\\dqn\\dqn_torch_policy.py\", line 148, in build_q_model_and_distribution\n",
      "\u001b[2m\u001b[36m(pid=10740)\u001b[0m     raise UnsupportedSpaceException(\n",
      "\u001b[2m\u001b[36m(pid=10740)\u001b[0m ray.rllib.utils.error.UnsupportedSpaceException: Action space Tuple(Discrete(7), Box(0.0, 1.0, (1,), float32), Discrete(4)) is not supported for DQN.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=10740)\u001b[0m Stream name: -29d8a821-9702-4a49-a3a6-ec06cc9e26c7\n"
     ]
    },
    {
     "ename": "RayActorError",
     "evalue": "The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=10740, ip=192.168.1.100)\n  File \"python\\ray\\_raylet.pyx\", line 501, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n  File \"D:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 563, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"D:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 536, in __init__\n    self.policy_map, self.preprocessors = self._build_policy_map(\n  File \"D:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1196, in _build_policy_map\n    policy_map[name] = cls(obs_space, act_space, merged_conf)\n  File \"D:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\policy\\policy_template.py\", line 236, in __init__\n    self.model, dist_class = make_model_and_action_dist(\n  File \"D:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\agents\\dqn\\dqn_torch_policy.py\", line 148, in build_q_model_and_distribution\n    raise UnsupportedSpaceException(\nray.rllib.utils.error.UnsupportedSpaceException: Action space Tuple(Discrete(7), Box(0.0, 1.0, (1,), float32), Discrete(4)) is not supported for DQN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e6f7896e1539>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQNTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'CuriosityLimitMarket'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\agents\\trainer_template.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m             \u001b[0mTrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         def _init(self, config: TrainerConfigDict,\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[0mlogger_creator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault_logger_creator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\tune\\trainable.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, logger_creator)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0msetup_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msetup_time\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mSETUP_TIME_THRESHOLD\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mget_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m             \u001b[1;31m# Evaluation setup.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\agents\\trainer_template.py\u001b[0m in \u001b[0;36m_init\u001b[1;34m(self, config, env_creator)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[1;31m# Creating all workers (excluding evaluation workers).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             self.workers = self._make_workers(\n\u001b[0m\u001b[0;32m    151\u001b[0m                 \u001b[0menv_creator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                 \u001b[0mvalidate_env\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_env\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\u001b[0m in \u001b[0;36m_make_workers\u001b[1;34m(self, env_creator, validate_env, policy_class, config, num_workers)\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mWorkerSet\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mWorkerSet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \"\"\"\n\u001b[1;32m--> 791\u001b[1;33m         return WorkerSet(\n\u001b[0m\u001b[0;32m    792\u001b[0m             \u001b[0menv_creator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mvalidate_env\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_env\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, logdir, _setup)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;31m# to not be forced to create an Env on the local worker.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_remote_workers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 remote_spaces = ray.get(self.remote_workers(\n\u001b[0m\u001b[0;32m     82\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforeach_policy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                     lambda p, pid: (pid, p.observation_space, p.action_space)))\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclient_mode_should_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\worker.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(object_refs, timeout)\u001b[0m\n\u001b[0;32m   1494\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1496\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_individual_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=10740, ip=192.168.1.100)\n  File \"python\\ray\\_raylet.pyx\", line 501, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 451, in ray._raylet.execute_task.function_executor\n  File \"D:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 563, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"D:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 536, in __init__\n    self.policy_map, self.preprocessors = self._build_policy_map(\n  File \"D:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 1196, in _build_policy_map\n    policy_map[name] = cls(obs_space, act_space, merged_conf)\n  File \"D:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\policy\\policy_template.py\", line 236, in __init__\n    self.model, dist_class = make_model_and_action_dist(\n  File \"D:\\Program_files\\Anaconda\\envs\\ray\\lib\\site-packages\\ray\\rllib\\agents\\dqn\\dqn_torch_policy.py\", line 148, in build_q_model_and_distribution\n    raise UnsupportedSpaceException(\nray.rllib.utils.error.UnsupportedSpaceException: Action space Tuple(Discrete(7), Box(0.0, 1.0, (1,), float32), Discrete(4)) is not supported for DQN."
     ]
    }
   ],
   "source": [
    "trainer = DQNTrainer(trainer_config, env = 'CuriosityLimitMarket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_reward = -np.inf\n",
    "trainer.save()\n",
    "hall_of_fame = [0]\n",
    "last_checkpoint = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(100):\n",
    "    print(\"Training iteration {}...\".format(i))\n",
    "    results = trainer.train()\n",
    "    this_reward = results['episode_reward_max']\n",
    "    if this_reward > best_reward:\n",
    "        best_reward = this_reward\n",
    "        trainer.save()\n",
    "        path = trainer.logdir + 'checkpoint_{0}/checkpoint-{0}'.format(last_checkpoint)\n",
    "        os.remove(path)\n",
    "        last_checkpoint = i + 1\n",
    "        hall_of_fame.append(i+1)\n",
    "        print('New best reward')\n",
    "        print(best_reward)\n",
    "    if i % 10 == 0:\n",
    "        print('Best Reward So Far')\n",
    "        print(best_reward)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = trainer.logdir + 'checkpoint_{0}/checkpoint-{0}'.format(hall_of_fame[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training = pd.read_csv(trainer.logdir + 'progress.csv')\n",
    "plt.plot(training['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LimitMarket(conf)\n",
    "obs = env.reset()\n",
    "\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "prices = []\n",
    "assets = []\n",
    "actions = []\n",
    "states = [obs]\n",
    "rewards = []\n",
    "hidden = [torch.zeros(512),torch.zeros(512)]\n",
    "infos = []\n",
    "while not done:\n",
    "    action, hidden, info = trainer.compute_action(obs, hidden)\n",
    "    obs, reward, done, results = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    assets.append(results['assets'])\n",
    "    prices.append(results['current_price'])\n",
    "    states.append(obs)\n",
    "    infos.append(info)\n",
    "    if i % 100 == 0:\n",
    "        print('Step: {}/{}'.format(i, 200))\n",
    "print(\"Cumulative reward you've received is: {}. Congratulations!\".format(cumulative_reward))\n",
    "print(\"Asset_Gain {}\".format(assets[-1] -assets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_actions = []\n",
    "for action in actions:\n",
    "    pure_actions.append(action[0])\n",
    "    \n",
    "actions = pure_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy10 = np.ma.masked_where(np.array(actions) != 0, prices)\n",
    "buy20 = np.ma.masked_where(np.array(actions) != 1, prices)\n",
    "buy50 = np.ma.masked_where(np.array(actions) != 2, prices)\n",
    "sell10 = np.ma.masked_where(np.array(actions) != 3, prices)\n",
    "sell20 = np.ma.masked_where(np.array(actions) != 4, prices)\n",
    "sell50 = np.ma.masked_where(np.array(actions) != 5, prices)\n",
    "hold = np.ma.masked_where(np.array(actions) != 6, prices)\n",
    "\n",
    "# plt.plot(prices, marker = '', markersize = 0.5, markevery = np.where(np.array(actions) == 6, True, False))\n",
    "# plt.figure(figsize = (20, 15))\n",
    "# plt.plot(buy10, c = 'turquoise', linewidth = 0.6)\n",
    "# plt.plot(buy20, c = 'lime', linewidth = 0.6)\n",
    "# plt.plot(buy50, c = 'green', linewidth = 0.6)\n",
    "# plt.plot(hold, c = 'blue', linewidth = 0.6)\n",
    "graph_prices = prices[::10][:500]\n",
    "graph_actions = actions[::10][:500]\n",
    "colors = ['r', 'r', 'r', 'g', 'g', 'g', 'b']\n",
    "fig = plt.figure(figsize = (10, 6))\n",
    "plt.scatter(range(len(graph_prices)), graph_prices, s=1, color = np.array(colors)[graph_actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.test()\n",
    "\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "prices = []\n",
    "assets = []\n",
    "actions = []\n",
    "states = [obs]\n",
    "rewards = []\n",
    "hidden = [torch.zeros(512),torch.zeros(512)]\n",
    "infos = []\n",
    "market_beaters = []\n",
    "while not done:\n",
    "    action, hidden, info = trainer.compute_action(obs, hidden)\n",
    "    obs, reward, done, results = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    assets.append(results['assets'])\n",
    "    prices.append(results['current_price'])\n",
    "    states.append(obs)\n",
    "    infos.append(info)\n",
    "    market_beaters.append(results['market_beater'])\n",
    "print(\"Cumulative reward you've received is: {}. Congratulations!\".format(cumulative_reward))\n",
    "print(\"Asset_Gain {}\".format(assets[-1] -assets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(market_beaters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_actions = []\n",
    "for action in actions:\n",
    "    print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy10 = np.ma.masked_where(np.array(actions) == 0, prices)\n",
    "buy20 = np.ma.masked_where(np.array(actions) == 1, prices)\n",
    "buy50 = np.ma.masked_where(np.array(actions) == 2, prices)\n",
    "hold = np.ma.masked_where(np.array(actions) == 3, prices)\n",
    "\n",
    "plt.plot(buy10, c = 'turquoise')\n",
    "plt.plot(buy20, c = 'lime')\n",
    "plt.plot(buy50, c = 'green')\n",
    "plt.plot(hold, c = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_beaters[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_norm = preprocessing.normalize(np.array(prices).reshape(-1,1))\n",
    "assets_norm = preprocessing.normalize(np.array(assets).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_norm-prices_norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
